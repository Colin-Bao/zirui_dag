nohup: ignoring input
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2022-10-12 10:46:28 +0800] [707815] [INFO] Starting gunicorn 20.1.0
[2022-10-12 10:46:28 +0800] [707815] [INFO] Listening at: http://[::]:8793 (707815)
[2022-10-12 10:46:28 +0800] [707815] [INFO] Using worker: sync
[[34m2022-10-12 10:46:28,351[0m] {[34mscheduler_job.py:[0m701} INFO[0m - Starting the scheduler[0m
[[34m2022-10-12 10:46:28,351[0m] {[34mscheduler_job.py:[0m706} INFO[0m - Processing each file at most -1 times[0m
[2022-10-12 10:46:28 +0800] [707816] [INFO] Booting worker with pid: 707816
[[34m2022-10-12 10:46:28,355[0m] {[34mexecutor_loader.py:[0m107} INFO[0m - Loaded executor: LocalExecutor[0m
[2022-10-12 10:46:28 +0800] [707877] [INFO] Booting worker with pid: 707877
[[34m2022-10-12 10:46:28,433[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 707953[0m
[[34m2022-10-12 10:46:28,433[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 10:46:28,435[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 10:46:28,450[0m] {[34mscheduler_job.py:[0m1392} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2022-10-12 10:51:28,484[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 10:56:28,513[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:01:28,541[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:06:28,566[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:11:28,594[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:16:28,630[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
Process ForkProcess-35:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:18:47,265[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=707953) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:18:47,268[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 970766[0m
[[34m2022-10-12 11:18:47,272[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:18:47.658+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 35, in create_session
    yield session
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-36:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
[[34m2022-10-12 11:18:48,535[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=970766) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:18:48,537[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 970819[0m
[[34m2022-10-12 11:18:48,541[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 11:21:28,658[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:26:28,684[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:31:28,713[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
Process ForkProcess-37:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:36:16,242[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=970819) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:36:16,246[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1117144[0m
[[34m2022-10-12 11:36:16,251[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 11:36:28,740[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
Process ForkProcess-38:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:37:33,664[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1117144) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:37:33,669[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1127399[0m
[[34m2022-10-12 11:37:33,676[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-39:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:39:48,325[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1127399) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:39:48,327[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1145369[0m
[[34m2022-10-12 11:39:48,331[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-40:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:40:05,233[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1145369) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:40:05,238[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1147352[0m
[[34m2022-10-12 11:40:05,244[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:40:24.498+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 35, in create_session
    yield session
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-41:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
[[34m2022-10-12 11:40:24,818[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1147352) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:40:24,823[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1149827[0m
[[34m2022-10-12 11:40:24,828[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-42:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:40:47,055[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1149827) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:40:47,061[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1152612[0m
[[34m2022-10-12 11:40:47,069[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:40:50.584+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 35, in create_session
    yield session
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-43:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
[[34m2022-10-12 11:40:51,354[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1152612) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:40:51,357[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1152950[0m
[[34m2022-10-12 11:40:51,362[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-44:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:41:23,462[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1152950) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:23,467[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1156488[0m
[[34m2022-10-12 11:41:23,474[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 11:41:28,767[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
Process ForkProcess-45:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:41:29,816[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1156488) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:29,819[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1157297[0m
[[34m2022-10-12 11:41:29,823[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-46:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:41:33,386[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1157297) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:33,391[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1157661[0m
[[34m2022-10-12 11:41:33,397[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:41:40.740+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 35, in create_session
    yield session
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-47:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
[[34m2022-10-12 11:41:41,492[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1157661) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:41,498[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1158508[0m
[[34m2022-10-12 11:41:41,502[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-48:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:41:44,600[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1158508) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:44,604[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1158809[0m
[[34m2022-10-12 11:41:44,608[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:41:58.885+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 35, in create_session
    yield session
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-49:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 83, in purge_inactive_dag_warnings
    session.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1451, in commit
    self._transaction.commit(_to_root=self.future)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 836, in commit
    trans.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2459, in commit
    self._do_commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2649, in _do_commit
    self._connection_commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2620, in _connection_commit_impl
    self.connection._commit_impl()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1091, in _commit_impl
    self._handle_dbapi_exception(e, None, None, None, None)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2128, in _handle_dbapi_exception
    util.raise_(exc_info[1], with_traceback=exc_info[2])
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1089, in _commit_impl
    self.engine.dialect.do_commit(self.connection)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 686, in do_commit
    dbapi_connection.commit()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 444, in commit
    self._cmysql.commit()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
[[34m2022-10-12 11:41:59,785[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1158809) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:41:59,789[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1160448[0m
[[34m2022-10-12 11:41:59,794[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-50:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:42:18,795[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1160448) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:42:18,800[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1162619[0m
[[34m2022-10-12 11:42:18,808[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-51:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:42:31,845[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1162619) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:42:31,851[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1163991[0m
[[34m2022-10-12 11:42:31,858[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-52:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:42:50,582[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1163991) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:42:50,588[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1166271[0m
[[34m2022-10-12 11:42:50,596[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-53:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:43:37,286[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1166271) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:43:37,291[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1171434[0m
[[34m2022-10-12 11:43:37,301[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2022-10-12T11:43:43.262+0800] {base.py:781} ERROR - Exception during reset or similar
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 753, in _finalize_fairy
    fairy._reset(pool)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1004, in _reset
    pool._dialect.do_rollback(self)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 683, in do_rollback
    dbapi_connection.rollback()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 450, in rollback
    self._cmysql.rollback()
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query
Process ForkProcess-54:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:44:14,569[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1171434) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:44:14,574[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1175672[0m
[[34m2022-10-12 11:44:14,578[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-55:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:44:39,189[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1175672) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:44:39,193[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1178033[0m
[[34m2022-10-12 11:44:39,199[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-56:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:45:28,002[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1178033) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:45:28,005[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1183392[0m
[[34m2022-10-12 11:45:28,010[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-57:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:45:30,358[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1183392) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:45:30,362[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1183504[0m
[[34m2022-10-12 11:45:30,373[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-58:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
mysql.connector.errors.OperationalError: MySQL Connection not available.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1806, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1800, in _execute_context
    context = constructor(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1019, in _init_compiled
    self.cursor = self.create_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1390, in create_cursor
    return self.create_default_cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 1393, in create_default_cursor
    return self._dbapi_connection.cursor()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/pool/base.py", line 1099, in cursor
    return self.dbapi_connection.cursor(*args, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 622, in cursor
    raise OperationalError("MySQL Connection not available.")
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) MySQL Connection not available.
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
[parameters: [{}]]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:45:57,184[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1183504) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:45:57,189[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1185990[0m
[[34m2022-10-12 11:45:57,193[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
Process ForkProcess-59:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 11:46:13,364[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1185990) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 11:46:13,370[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1187449[0m
[[34m2022-10-12 11:46:13,377[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 11:46:28,800[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:51:28,827[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 11:56:28,854[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:01:28,881[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:06:28,909[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:11:28,946[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:16:28,981[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:21:29,016[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:26:29,043[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:31:29,070[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 16 tasks up for execution:
	<TaskInstance: csc_ops_load.E_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 3/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 4/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 5/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 6/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 7/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,947[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 8/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 9/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 10/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 11/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 12/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 13/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 14/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 15/16 running and queued tasks[0m
[[34m2022-10-12 12:34:28,948[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_load.E_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.E_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:28,950[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREBALANCESHEET', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,950[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREBALANCESHEET', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHARECASHFLOW', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHARECASHFLOW', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREDIVIDEND', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREDIVIDEND', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREEODDERIVATIVEINDICATOR', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEODDERIVATIVEINDICATOR', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREEODPRICES', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEODPRICES', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREEXRIGHTDIVIDENDRECORD', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEXRIGHTDIVIDENDRECORD', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREINCOME', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREINCOME', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREPROFITEXPRESS', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,951[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREPROFITEXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,951[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_ASHAREPROFITNOTICE', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREPROFITNOTICE', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_BAS_STK_HISDISTRIBUTION', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_BAS_STK_HISDISTRIBUTION', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_FIN_BALANCE_SHEET_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_BALANCE_SHEET_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_FIN_CASH_FLOW_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_CASH_FLOW_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_FIN_INCOME_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_INCOME_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_FIN_PERFORMANCE_EXPRESS', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_PERFORMANCE_EXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_FIN_PERFORMANCE_FORECAST', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_PERFORMANCE_FORECAST', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,952[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='E_QT_STK_DAILY', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:28,952[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_QT_STK_DAILY', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREBALANCESHEET', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHARECASHFLOW', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREDIVIDEND', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEODDERIVATIVEINDICATOR', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEODPRICES', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,955[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREEXRIGHTDIVIDENDRECORD', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,956[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREINCOME', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,956[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREPROFITEXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,956[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_BAS_STK_HISDISTRIBUTION', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,956[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_ASHAREPROFITNOTICE', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,956[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_BALANCE_SHEET_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,958[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_INCOME_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,958[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_CASH_FLOW_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,958[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_PERFORMANCE_EXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,959[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_FIN_PERFORMANCE_FORECAST', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:28,964[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'E_QT_STK_DAILY', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:29,161[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,164[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,166[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,172[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,172[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,180[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,182[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,183[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,184[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,188[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,190[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,192[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,193[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,199[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,218[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:29,284[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.E_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 15 tasks up for execution:
	<TaskInstance: csc_ops_load.L_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 3/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 4/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 5/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 6/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 7/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 8/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 9/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,403[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 10/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,404[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 11/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,404[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 12/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,404[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 13/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,404[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 14/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,404[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_load.L_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.L_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:30,406[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREBALANCESHEET', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREBALANCESHEET', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHARECASHFLOW', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHARECASHFLOW', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREEODDERIVATIVEINDICATOR', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEODDERIVATIVEINDICATOR', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREEODPRICES', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEODPRICES', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREEXRIGHTDIVIDENDRECORD', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEXRIGHTDIVIDENDRECORD', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREINCOME', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREINCOME', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREPROFITEXPRESS', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREPROFITEXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,407[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREPROFITNOTICE', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,407[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREPROFITNOTICE', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_BAS_STK_HISDISTRIBUTION', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_BAS_STK_HISDISTRIBUTION', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_FIN_BALANCE_SHEET_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_BALANCE_SHEET_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_FIN_CASH_FLOW_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_CASH_FLOW_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_FIN_INCOME_GEN', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_INCOME_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_FIN_PERFORMANCE_EXPRESS', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_PERFORMANCE_EXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_FIN_PERFORMANCE_FORECAST', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_PERFORMANCE_FORECAST', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,408[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_QT_STK_DAILY', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,408[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_QT_STK_DAILY', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,411[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREBALANCESHEET', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHARECASHFLOW', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEODDERIVATIVEINDICATOR', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEODPRICES', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREEXRIGHTDIVIDENDRECORD', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREINCOME', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREPROFITEXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREPROFITNOTICE', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_BAS_STK_HISDISTRIBUTION', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,412[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_BALANCE_SHEET_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,413[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_CASH_FLOW_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,413[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_PERFORMANCE_EXPRESS', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,414[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_PERFORMANCE_FORECAST', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_FIN_CASH_FLOW_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHARECASHFLOW run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_FIN_BALANCE_SHEET_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREINCOME run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_BAS_STK_HISDISTRIBUTION run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREPROFITNOTICE run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREPROFITEXPRESS run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,415[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREEXRIGHTDIVIDENDRECORD run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_QT_STK_DAILY run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREBALANCESHEET run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREEODDERIVATIVEINDICATOR run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_FIN_INCOME_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_FIN_PERFORMANCE_EXPRESS run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_FIN_PERFORMANCE_FORECAST run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,416[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_FIN_INCOME_GEN', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,416[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_QT_STK_DAILY', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,443[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREBALANCESHEET, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.280982+00:00, run_end_date=2022-10-12 04:34:30.146802+00:00, run_duration=0.86582, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23413, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586701[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHARECASHFLOW, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.253802+00:00, run_end_date=2022-10-12 04:34:30.079554+00:00, run_duration=0.825752, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23406, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586690[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREEODDERIVATIVEINDICATOR, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.284575+00:00, run_end_date=2022-10-12 04:34:30.153343+00:00, run_duration=0.868768, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23409, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586700[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREEXRIGHTDIVIDENDRECORD, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.280428+00:00, run_end_date=2022-10-12 04:34:30.130723+00:00, run_duration=0.850295, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23416, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586695[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREINCOME, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.250644+00:00, run_end_date=2022-10-12 04:34:30.113847+00:00, run_duration=0.863203, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23405, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586689[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREPROFITEXPRESS, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.315845+00:00, run_end_date=2022-10-12 04:34:30.142024+00:00, run_duration=0.826179, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23414, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586708[0m
[[34m2022-10-12 12:34:30,444[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREPROFITNOTICE, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.260136+00:00, run_end_date=2022-10-12 04:34:30.116399+00:00, run_duration=0.856263, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23408, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586692[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_BAS_STK_HISDISTRIBUTION, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.280219+00:00, run_end_date=2022-10-12 04:34:30.109465+00:00, run_duration=0.829246, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23404, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586698[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_FIN_BALANCE_SHEET_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.263721+00:00, run_end_date=2022-10-12 04:34:30.097335+00:00, run_duration=0.833614, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23410, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586693[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_FIN_CASH_FLOW_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.253837+00:00, run_end_date=2022-10-12 04:34:30.074441+00:00, run_duration=0.820604, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23407, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586691[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_FIN_INCOME_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.284136+00:00, run_end_date=2022-10-12 04:34:30.139202+00:00, run_duration=0.855066, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23417, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586702[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_FIN_PERFORMANCE_EXPRESS, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.376683+00:00, run_end_date=2022-10-12 04:34:30.178483+00:00, run_duration=0.8018, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23419, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586720[0m
[[34m2022-10-12 12:34:30,445[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_FIN_PERFORMANCE_FORECAST, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.319766+00:00, run_end_date=2022-10-12 04:34:30.150476+00:00, run_duration=0.83071, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23418, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586707[0m
[[34m2022-10-12 12:34:30,446[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_QT_STK_DAILY, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.280850+00:00, run_end_date=2022-10-12 04:34:30.146248+00:00, run_duration=0.865398, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23415, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586704[0m
[[34m2022-10-12 12:34:30,588[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: csc_ops_load.L_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:30,588[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 15/16 running and queued tasks[0m
[[34m2022-10-12 12:34:30,588[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_load.L_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:30,591[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='L_ASHAREDIVIDEND', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:30,591[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREDIVIDEND', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,602[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'L_ASHAREDIVIDEND', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:30,602[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREEODPRICES run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,602[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.E_ASHAREDIVIDEND run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:30,621[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREDIVIDEND, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.291190+00:00, run_end_date=2022-10-12 04:34:30.183085+00:00, run_duration=0.891895, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23412, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586703[0m
[[34m2022-10-12 12:34:30,621[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=E_ASHAREEODPRICES, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:29.271081+00:00, run_end_date=2022-10-12 04:34:30.161221+00:00, run_duration=0.89014, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23411, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:28.948520+00:00, queued_by_job_id=23403, pid=1586694[0m
[[34m2022-10-12 12:34:30,660[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHARECASHFLOW manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,665[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREINCOME manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,665[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREPROFITEXPRESS manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,680[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_EXPRESS manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,680[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_FIN_CASH_FLOW_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,683[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREBALANCESHEET manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,685[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_FIN_BALANCE_SHEET_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,686[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREEXRIGHTDIVIDENDRECORD manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,689[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREEODDERIVATIVEINDICATOR manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,691[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_QT_STK_DAILY manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,692[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREPROFITNOTICE manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,704[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREEODPRICES manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,706[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_FIN_PERFORMANCE_FORECAST manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,711[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_FIN_INCOME_GEN manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,747[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_BAS_STK_HISDISTRIBUTION manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:30,857[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.L_ASHAREDIVIDEND manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 16 tasks up for execution:
	<TaskInstance: csc_ops_load.send_info manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__1 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__10 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__11 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__12 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__13 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__14 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__15 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__2 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__3 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__4 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__5 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__6 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__7 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__8 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__9 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 3/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 4/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 5/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 6/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,953[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 7/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 8/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 9/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 10/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 11/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 12/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 13/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 14/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_load has 15/16 running and queued tasks[0m
[[34m2022-10-12 12:34:32,954[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_load.send_info manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__1 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__10 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__11 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__12 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__13 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__14 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__15 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__2 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__3 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__4 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__5 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__6 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__7 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__8 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>
	<TaskInstance: csc_ops_load.send_info__9 manual__2022-10-12T04:34:27.004397+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:32,956[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,956[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,956[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__1', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,956[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__1', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,956[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__10', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__10', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__11', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__11', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__12', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__12', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__13', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__13', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__14', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__14', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__15', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__15', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__2', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__2', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__3', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__3', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,957[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__4', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,957[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__4', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,958[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__5', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,958[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__5', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,958[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__6', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,958[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__6', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,958[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__7', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,958[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__7', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,958[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__8', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,958[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__8', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,958[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_load', task_id='send_info__9', run_id='manual__2022-10-12T04:34:27.004397+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:32,958[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__9', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__1', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__10', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__11', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__13', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__12', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__14', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,961[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__15', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__2', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__3', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__4', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__5', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,962[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__6', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,963[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__7', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,968[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__8', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,968[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_load', 'send_info__9', 'manual__2022-10-12T04:34:27.004397+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_load.py'][0m
[[34m2022-10-12 12:34:32,971[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHARECASHFLOW run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREINCOME run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_FIN_INCOME_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_FIN_PERFORMANCE_EXPRESS run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREEXRIGHTDIVIDENDRECORD run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_FIN_BALANCE_SHEET_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_FIN_CASH_FLOW_GEN run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREPROFITEXPRESS run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREBALANCESHEET run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_FIN_PERFORMANCE_FORECAST run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_BAS_STK_HISDISTRIBUTION run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,972[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREPROFITNOTICE run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,973[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREEODPRICES run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,973[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREDIVIDEND run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,973[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_QT_STK_DAILY run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,973[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.L_ASHAREEODDERIVATIVEINDICATOR run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:32,980[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREBALANCESHEET, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.778136+00:00, run_end_date=2022-10-12 04:34:32.176915+00:00, run_duration=1.39878, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23425, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586860[0m
[[34m2022-10-12 12:34:32,980[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHARECASHFLOW, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.760434+00:00, run_end_date=2022-10-12 04:34:32.042619+00:00, run_duration=1.28218, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23420, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586857[0m
[[34m2022-10-12 12:34:32,980[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREDIVIDEND, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.945255+00:00, run_end_date=2022-10-12 04:34:32.435245+00:00, run_duration=1.48999, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23435, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.589383+00:00, queued_by_job_id=23403, pid=1586894[0m
[[34m2022-10-12 12:34:32,981[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREEODDERIVATIVEINDICATOR, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.778494+00:00, run_end_date=2022-10-12 04:34:32.708094+00:00, run_duration=1.9296, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23428, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586870[0m
[[34m2022-10-12 12:34:32,981[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREEODPRICES, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.788241+00:00, run_end_date=2022-10-12 04:34:32.414743+00:00, run_duration=1.6265, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23431, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586862[0m
[[34m2022-10-12 12:34:32,981[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREEXRIGHTDIVIDENDRECORD, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.778136+00:00, run_end_date=2022-10-12 04:34:32.114724+00:00, run_duration=1.33659, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23426, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586875[0m
[[34m2022-10-12 12:34:32,981[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREINCOME, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.748606+00:00, run_end_date=2022-10-12 04:34:32.050044+00:00, run_duration=1.30144, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23422, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586853[0m
[[34m2022-10-12 12:34:32,981[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREPROFITEXPRESS, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.783498+00:00, run_end_date=2022-10-12 04:34:32.200597+00:00, run_duration=1.4171, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23421, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586880[0m
[[34m2022-10-12 12:34:32,982[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_ASHAREPROFITNOTICE, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.778595+00:00, run_end_date=2022-10-12 04:34:32.331749+00:00, run_duration=1.55315, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23429, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586858[0m
[[34m2022-10-12 12:34:32,982[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_BAS_STK_HISDISTRIBUTION, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.847569+00:00, run_end_date=2022-10-12 04:34:32.267932+00:00, run_duration=1.42036, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23434, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586885[0m
[[34m2022-10-12 12:34:32,982[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_FIN_BALANCE_SHEET_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.779264+00:00, run_end_date=2022-10-12 04:34:32.155493+00:00, run_duration=1.37623, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23427, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586861[0m
[[34m2022-10-12 12:34:32,982[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_FIN_CASH_FLOW_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.778743+00:00, run_end_date=2022-10-12 04:34:32.152622+00:00, run_duration=1.37388, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23424, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586866[0m
[[34m2022-10-12 12:34:32,982[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_FIN_INCOME_GEN, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.801017+00:00, run_end_date=2022-10-12 04:34:32.087554+00:00, run_duration=1.28654, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23433, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586864[0m
[[34m2022-10-12 12:34:32,983[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_FIN_PERFORMANCE_EXPRESS, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.790329+00:00, run_end_date=2022-10-12 04:34:32.103098+00:00, run_duration=1.31277, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23423, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586884[0m
[[34m2022-10-12 12:34:32,983[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_FIN_PERFORMANCE_FORECAST, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.806590+00:00, run_end_date=2022-10-12 04:34:32.237720+00:00, run_duration=1.43113, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23432, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586867[0m
[[34m2022-10-12 12:34:32,983[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=L_QT_STK_DAILY, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:30.800372+00:00, run_end_date=2022-10-12 04:34:32.466540+00:00, run_duration=1.66617, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23430, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:30.404491+00:00, queued_by_job_id=23403, pid=1586863[0m
[[34m2022-10-12 12:34:33,206[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__14 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,207[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 16 tasks up for execution:
	<TaskInstance: csc_income_merge.T_FIN_INCOME_GEN dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.T_ASHAREINCOME dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.T_ASHARECASHFLOW dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.T_FIN_CASH_FLOW_GEN dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.T_FIN_BALANCE_SHEET_GEN dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.T_ASHAREBALANCESHEET dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.T_FIN_PERFORMANCE_EXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.T_ASHAREPROFITEXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.T_FIN_PERFORMANCE_FORECAST dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.T_ASHAREPROFITNOTICE dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_BAS_STK_HISDISTRIBUTION dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_ASHAREEXRIGHTDIVIDENDRECORD dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_ASHAREDIVIDEND dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.T_ASHAREEODPRICES dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>
	<TaskInstance: csc_derivative_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_income_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_income_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_cashflow_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_cashflow_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_balance_sheet_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_balance_sheet_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_express_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_express_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_notice_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,208[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_notice_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_dividend_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_dividend_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_dividend_merge has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_prices_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_prices_merge has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_derivative_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_income_merge.T_FIN_INCOME_GEN dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.T_ASHAREINCOME dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.T_ASHARECASHFLOW dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.T_FIN_CASH_FLOW_GEN dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.T_FIN_BALANCE_SHEET_GEN dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.T_ASHAREBALANCESHEET dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.T_FIN_PERFORMANCE_EXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.T_ASHAREPROFITEXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.T_FIN_PERFORMANCE_FORECAST dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.T_ASHAREPROFITNOTICE dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_BAS_STK_HISDISTRIBUTION dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_ASHAREEXRIGHTDIVIDENDRECORD dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.T_ASHAREDIVIDEND dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.T_ASHAREEODPRICES dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>
	<TaskInstance: csc_derivative_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:33,209[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__2 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,211[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__6 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,214[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__4 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,216[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_income_merge', task_id='T_FIN_INCOME_GEN', run_id='dataset_triggered__2022-10-12T04:34:32.116082+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,214[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,216[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_income_merge', 'T_FIN_INCOME_GEN', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,217[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_income_merge', task_id='T_ASHAREINCOME', run_id='dataset_triggered__2022-10-12T04:34:32.116082+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,217[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_income_merge', 'T_ASHAREINCOME', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,218[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_cashflow_merge', task_id='T_ASHARECASHFLOW', run_id='dataset_triggered__2022-10-12T04:34:32.180080+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,218[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'T_ASHARECASHFLOW', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,218[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_cashflow_merge', task_id='T_FIN_CASH_FLOW_GEN', run_id='dataset_triggered__2022-10-12T04:34:32.180080+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,218[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'T_FIN_CASH_FLOW_GEN', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,218[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_balance_sheet_merge', task_id='T_FIN_BALANCE_SHEET_GEN', run_id='dataset_triggered__2022-10-12T04:34:32.203373+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,219[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'T_FIN_BALANCE_SHEET_GEN', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,219[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_balance_sheet_merge', task_id='T_ASHAREBALANCESHEET', run_id='dataset_triggered__2022-10-12T04:34:32.203373+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,219[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'T_ASHAREBALANCESHEET', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,219[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_express_merge', task_id='T_FIN_PERFORMANCE_EXPRESS', run_id='dataset_triggered__2022-10-12T04:34:32.226063+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,219[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'T_FIN_PERFORMANCE_EXPRESS', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,219[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_express_merge', task_id='T_ASHAREPROFITEXPRESS', run_id='dataset_triggered__2022-10-12T04:34:32.226063+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,219[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'T_ASHAREPROFITEXPRESS', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,219[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_notice_merge', task_id='T_FIN_PERFORMANCE_FORECAST', run_id='dataset_triggered__2022-10-12T04:34:32.349896+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,219[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'T_FIN_PERFORMANCE_FORECAST', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,220[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_notice_merge', task_id='T_ASHAREPROFITNOTICE', run_id='dataset_triggered__2022-10-12T04:34:32.349896+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,220[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'T_ASHAREPROFITNOTICE', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,220[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_dividend_merge', task_id='T_BAS_STK_HISDISTRIBUTION', run_id='dataset_triggered__2022-10-12T04:34:32.449117+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,220[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_BAS_STK_HISDISTRIBUTION', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,220[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_dividend_merge', task_id='T_ASHAREEXRIGHTDIVIDENDRECORD', run_id='dataset_triggered__2022-10-12T04:34:32.449117+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,220[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_ASHAREEXRIGHTDIVIDENDRECORD', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,220[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_dividend_merge', task_id='T_ASHAREDIVIDEND', run_id='dataset_triggered__2022-10-12T04:34:32.449117+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,220[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_ASHAREDIVIDEND', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,220[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_prices_merge', task_id='T_ASHAREEODPRICES', run_id='dataset_triggered__2022-10-12T04:34:32.485080+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,220[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_prices_merge', 'T_ASHAREEODPRICES', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,221[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_prices_merge', task_id='T_QT_STK_DAILY', run_id='dataset_triggered__2022-10-12T04:34:32.485080+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,221[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_prices_merge', 'T_QT_STK_DAILY', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,221[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_derivative_merge', task_id='T_QT_STK_DAILY', run_id='dataset_triggered__2022-10-12T04:34:32.721706+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:33,221[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'T_QT_STK_DAILY', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,221[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__12 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,224[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__15 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,224[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__10 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,232[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_income_merge', 'T_FIN_INCOME_GEN', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,232[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_income_merge', 'T_ASHAREINCOME', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,237[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'T_ASHARECASHFLOW', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,238[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__1 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,242[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'T_FIN_CASH_FLOW_GEN', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,242[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'T_FIN_BALANCE_SHEET_GEN', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,242[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'T_ASHAREBALANCESHEET', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,242[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'T_FIN_PERFORMANCE_EXPRESS', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,244[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__11 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,247[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'T_ASHAREPROFITEXPRESS', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,247[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'T_FIN_PERFORMANCE_FORECAST', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,247[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'T_ASHAREPROFITNOTICE', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,250[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_ASHAREEXRIGHTDIVIDENDRECORD', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,252[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_BAS_STK_HISDISTRIBUTION', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,253[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__5 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,254[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'T_ASHAREDIVIDEND', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,259[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__7 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,259[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_prices_merge', 'T_ASHAREEODPRICES', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,259[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_prices_merge', 'T_QT_STK_DAILY', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,260[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'T_QT_STK_DAILY', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:33,264[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__3 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,277[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__13 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,293[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__9 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,320[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_load.send_info__8 manual__2022-10-12T04:34:27.004397+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,579[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_cashflow_merge.T_FIN_CASH_FLOW_GEN dataset_triggered__2022-10-12T04:34:32.180080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,598[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_dividend_merge.T_BAS_STK_HISDISTRIBUTION dataset_triggered__2022-10-12T04:34:32.449117+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,625[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_income_merge.T_ASHAREINCOME dataset_triggered__2022-10-12T04:34:32.116082+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,643[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_express_merge.T_FIN_PERFORMANCE_EXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,651[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_dividend_merge.T_ASHAREDIVIDEND dataset_triggered__2022-10-12T04:34:32.449117+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,658[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_express_merge.T_ASHAREPROFITEXPRESS dataset_triggered__2022-10-12T04:34:32.226063+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,658[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_cashflow_merge.T_ASHARECASHFLOW dataset_triggered__2022-10-12T04:34:32.180080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,668[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_dividend_merge.T_ASHAREEXRIGHTDIVIDENDRECORD dataset_triggered__2022-10-12T04:34:32.449117+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,678[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_income_merge.T_FIN_INCOME_GEN dataset_triggered__2022-10-12T04:34:32.116082+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,690[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_notice_merge.T_FIN_PERFORMANCE_FORECAST dataset_triggered__2022-10-12T04:34:32.349896+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,697[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_balance_sheet_merge.T_ASHAREBALANCESHEET dataset_triggered__2022-10-12T04:34:32.203373+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,713[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_prices_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.485080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,763[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_notice_merge.T_ASHAREPROFITNOTICE dataset_triggered__2022-10-12T04:34:32.349896+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,795[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_derivative_merge.T_QT_STK_DAILY dataset_triggered__2022-10-12T04:34:32.721706+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,814[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_prices_merge.T_ASHAREEODPRICES dataset_triggered__2022-10-12T04:34:32.485080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:33,816[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_balance_sheet_merge.T_FIN_BALANCE_SHEET_GEN dataset_triggered__2022-10-12T04:34:32.203373+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:35,569[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_express_merge.T_ASHAREPROFITEXPRESS run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_notice_merge.T_FIN_PERFORMANCE_FORECAST run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_dividend_merge.T_BAS_STK_HISDISTRIBUTION run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_cashflow_merge.T_FIN_CASH_FLOW_GEN run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_dividend_merge.T_ASHAREEXRIGHTDIVIDENDRECORD run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_notice_merge.T_ASHAREPROFITNOTICE run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_derivative_merge.T_QT_STK_DAILY run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_balance_sheet_merge.T_FIN_BALANCE_SHEET_GEN run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_income_merge.T_ASHAREINCOME run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_income_merge.T_FIN_INCOME_GEN run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_balance_sheet_merge.T_ASHAREBALANCESHEET run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_prices_merge.T_QT_STK_DAILY run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_dividend_merge.T_ASHAREDIVIDEND run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,570[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_cashflow_merge.T_ASHARECASHFLOW run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:35,578[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_balance_sheet_merge, task_id=T_ASHAREBALANCESHEET, run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.879487+00:00, run_end_date=2022-10-12 04:34:35.417309+00:00, run_duration=1.53782, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23462, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587168[0m
[[34m2022-10-12 12:34:35,578[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_balance_sheet_merge, task_id=T_FIN_BALANCE_SHEET_GEN, run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.997152+00:00, run_end_date=2022-10-12 04:34:35.386271+00:00, run_duration=1.38912, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23467, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587173[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_cashflow_merge, task_id=T_ASHARECASHFLOW, run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.864422+00:00, run_end_date=2022-10-12 04:34:35.422377+00:00, run_duration=1.55796, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23457, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587166[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_cashflow_merge, task_id=T_FIN_CASH_FLOW_GEN, run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.697560+00:00, run_end_date=2022-10-12 04:34:35.358292+00:00, run_duration=1.66073, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23452, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587152[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_derivative_merge, task_id=T_QT_STK_DAILY, run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.933563+00:00, run_end_date=2022-10-12 04:34:35.374720+00:00, run_duration=1.44116, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23465, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587171[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_dividend_merge, task_id=T_ASHAREDIVIDEND, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.803623+00:00, run_end_date=2022-10-12 04:34:35.427795+00:00, run_duration=1.62417, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23456, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587158[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_dividend_merge, task_id=T_ASHAREEXRIGHTDIVIDENDRECORD, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.811385+00:00, run_end_date=2022-10-12 04:34:35.358292+00:00, run_duration=1.54691, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23459, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587160[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_dividend_merge, task_id=T_BAS_STK_HISDISTRIBUTION, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.745946+00:00, run_end_date=2022-10-12 04:34:35.336344+00:00, run_duration=1.5904, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23453, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587155[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_income_merge, task_id=T_ASHAREINCOME, run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.760594+00:00, run_end_date=2022-10-12 04:34:35.374978+00:00, run_duration=1.61438, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23454, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587157[0m
[[34m2022-10-12 12:34:35,579[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_income_merge, task_id=T_FIN_INCOME_GEN, run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.873372+00:00, run_end_date=2022-10-12 04:34:35.392405+00:00, run_duration=1.51903, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23460, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587170[0m
[[34m2022-10-12 12:34:35,580[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_prices_merge, task_id=T_QT_STK_DAILY, run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.868774+00:00, run_end_date=2022-10-12 04:34:35.422727+00:00, run_duration=1.55395, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23463, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587169[0m
[[34m2022-10-12 12:34:35,580[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_express_merge, task_id=T_ASHAREPROFITEXPRESS, run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.794724+00:00, run_end_date=2022-10-12 04:34:35.329902+00:00, run_duration=1.53518, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23458, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587161[0m
[[34m2022-10-12 12:34:35,580[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_notice_merge, task_id=T_ASHAREPROFITNOTICE, run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.916753+00:00, run_end_date=2022-10-12 04:34:35.366567+00:00, run_duration=1.44981, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23464, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587172[0m
[[34m2022-10-12 12:34:35,580[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_notice_merge, task_id=T_FIN_PERFORMANCE_FORECAST, run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.832584+00:00, run_end_date=2022-10-12 04:34:35.338162+00:00, run_duration=1.50558, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23461, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587164[0m
[[34m2022-10-12 12:34:36,495[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_ops_load @ 2022-10-12 04:34:27.004397+00:00: manual__2022-10-12T04:34:27.004397+00:00, state:running, queued_at: 2022-10-12 04:34:27.019382+00:00. externally triggered: True> successful[0m
[[34m2022-10-12 12:34:36,495[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_ops_load, execution_date=2022-10-12 04:34:27.004397+00:00, run_id=manual__2022-10-12T04:34:27.004397+00:00, run_start_date=2022-10-12 04:34:28.913523+00:00, run_end_date=2022-10-12 04:34:36.495482+00:00, run_duration=7.581959, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=bb1f0c1446de05393fbf495b6ac9e192[0m
[[34m2022-10-12 12:34:36,499[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_ops_load to 2022-10-11T17:00:00+00:00, run_after=2022-10-12T17:00:00+00:00[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 8 tasks up for execution:
	<TaskInstance: csc_derivative_merge.T_ASHAREEODDERIVATIVEINDICATOR dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.M_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.M_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.M_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.M_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.M_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.M_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.M_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_derivative_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_income_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_cashflow_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_balance_sheet_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_express_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_notice_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_dividend_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_prices_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:36,547[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_derivative_merge.T_ASHAREEODDERIVATIVEINDICATOR dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.M_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.M_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.M_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.M_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.M_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.M_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.M_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_derivative_merge', task_id='T_ASHAREEODDERIVATIVEINDICATOR', run_id='dataset_triggered__2022-10-12T04:34:32.721706+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'T_ASHAREEODDERIVATIVEINDICATOR', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_income_merge', task_id='M_CSC_Income', run_id='dataset_triggered__2022-10-12T04:34:32.116082+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_income_merge', 'M_CSC_Income', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_cashflow_merge', task_id='M_CSC_CashFlow', run_id='dataset_triggered__2022-10-12T04:34:32.180080+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'M_CSC_CashFlow', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_balance_sheet_merge', task_id='M_CSC_Balance_Sheet', run_id='dataset_triggered__2022-10-12T04:34:32.203373+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'M_CSC_Balance_Sheet', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_express_merge', task_id='M_CSC_Profit_Express', run_id='dataset_triggered__2022-10-12T04:34:32.226063+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,549[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'M_CSC_Profit_Express', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,549[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_notice_merge', task_id='M_CSC_Profit_Notice', run_id='dataset_triggered__2022-10-12T04:34:32.349896+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,550[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'M_CSC_Profit_Notice', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,550[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_dividend_merge', task_id='M_CSC_Dividend', run_id='dataset_triggered__2022-10-12T04:34:32.449117+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,550[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'M_CSC_Dividend', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,550[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_prices_merge', task_id='M_CSC_Prices', run_id='dataset_triggered__2022-10-12T04:34:32.485080+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:36,550[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_prices_merge', 'M_CSC_Prices', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'T_ASHAREEODDERIVATIVEINDICATOR', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_income_merge', 'M_CSC_Income', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'M_CSC_CashFlow', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'M_CSC_Balance_Sheet', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'M_CSC_Profit_Express', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'M_CSC_Profit_Notice', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'M_CSC_Dividend', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,554[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_prices_merge', 'M_CSC_Prices', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__13 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_express_merge.T_FIN_PERFORMANCE_EXPRESS run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_prices_merge.T_ASHAREEODPRICES run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__4 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__10 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__5 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__15 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__12 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__9 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__6 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__7 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__1 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__11 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__14 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__3 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__2 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,556[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_load.send_info__8 run_id=manual__2022-10-12T04:34:27.004397+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:36,563[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.357328+00:00, run_end_date=2022-10-12 04:34:35.915892+00:00, run_duration=2.55856, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23440, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587141[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__1, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.439955+00:00, run_end_date=2022-10-12 04:34:35.953393+00:00, run_duration=2.51344, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23444, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587153[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__10, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.362589+00:00, run_end_date=2022-10-12 04:34:35.559275+00:00, run_duration=2.19669, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23443, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587135[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__11, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.415038+00:00, run_end_date=2022-10-12 04:34:36.005846+00:00, run_duration=2.59081, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23445, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587151[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__12, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.361736+00:00, run_end_date=2022-10-12 04:34:35.827324+00:00, run_duration=2.46559, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23441, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587146[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__13, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.395620+00:00, run_end_date=2022-10-12 04:34:35.491434+00:00, run_duration=2.09581, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23449, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587121[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__14, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.363831+00:00, run_end_date=2022-10-12 04:34:36.019720+00:00, run_duration=2.65589, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23436, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587148[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__15, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.362755+00:00, run_end_date=2022-10-12 04:34:35.759663+00:00, run_duration=2.39691, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23442, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587134[0m
[[34m2022-10-12 12:34:36,564[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__2, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.350039+00:00, run_end_date=2022-10-12 04:34:36.037189+00:00, run_duration=2.68715, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23437, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587145[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__3, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.483484+00:00, run_end_date=2022-10-12 04:34:36.006606+00:00, run_duration=2.52312, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23447, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587162[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__4, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.350008+00:00, run_end_date=2022-10-12 04:34:35.547833+00:00, run_duration=2.19782, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23439, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587125[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__5, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.370570+00:00, run_end_date=2022-10-12 04:34:35.638381+00:00, run_duration=2.26781, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23446, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587124[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__6, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.360981+00:00, run_end_date=2022-10-12 04:34:35.879718+00:00, run_duration=2.51874, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23438, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587129[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__7, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.435824+00:00, run_end_date=2022-10-12 04:34:35.943096+00:00, run_duration=2.50727, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23448, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587132[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__8, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.475842+00:00, run_end_date=2022-10-12 04:34:36.151443+00:00, run_duration=2.6756, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23451, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587159[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_load, task_id=send_info__9, run_id=manual__2022-10-12T04:34:27.004397+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.535691+00:00, run_end_date=2022-10-12 04:34:35.855912+00:00, run_duration=2.32022, state=success, executor_state=success, try_number=1, max_tries=1, job_id=23450, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:32.954605+00:00, queued_by_job_id=23403, pid=1587139[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_prices_merge, task_id=T_ASHAREEODPRICES, run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:34.042290+00:00, run_end_date=2022-10-12 04:34:35.509959+00:00, run_duration=1.46767, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23466, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587175[0m
[[34m2022-10-12 12:34:36,565[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_express_merge, task_id=T_FIN_PERFORMANCE_EXPRESS, run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00, map_index=-1, run_start_date=2022-10-12 04:34:33.765478+00:00, run_end_date=2022-10-12 04:34:35.477937+00:00, run_duration=1.71246, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23455, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:33.210279+00:00, queued_by_job_id=23403, pid=1587154[0m
[[34m2022-10-12 12:34:36,713[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_dividend_merge.M_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,723[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_balance_sheet_merge.M_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,725[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_express_merge.M_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,731[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_cashflow_merge.M_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,741[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_prices_merge.M_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,747[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_income_merge.M_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,754[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_notice_merge.M_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:36,758[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_derivative_merge.T_ASHAREEODDERIVATIVEINDICATOR dataset_triggered__2022-10-12T04:34:32.721706+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 8 tasks up for execution:
	<TaskInstance: csc_derivative_merge.M_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.C_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.C_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.C_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.C_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.C_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.C_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.C_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_derivative_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_income_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_cashflow_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_balance_sheet_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_express_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_profit_notice_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_dividend_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_prices_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:37,719[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_derivative_merge.M_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>
	<TaskInstance: csc_income_merge.C_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [scheduled]>
	<TaskInstance: csc_cashflow_merge.C_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [scheduled]>
	<TaskInstance: csc_balance_sheet_merge.C_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [scheduled]>
	<TaskInstance: csc_profit_express_merge.C_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [scheduled]>
	<TaskInstance: csc_profit_notice_merge.C_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [scheduled]>
	<TaskInstance: csc_dividend_merge.C_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [scheduled]>
	<TaskInstance: csc_prices_merge.C_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_derivative_merge', task_id='M_CSC_Derivative', run_id='dataset_triggered__2022-10-12T04:34:32.721706+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'M_CSC_Derivative', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_income_merge', task_id='C_CSC_Income', run_id='dataset_triggered__2022-10-12T04:34:32.116082+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_income_merge', 'C_CSC_Income', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_cashflow_merge', task_id='C_CSC_CashFlow', run_id='dataset_triggered__2022-10-12T04:34:32.180080+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'C_CSC_CashFlow', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_balance_sheet_merge', task_id='C_CSC_Balance_Sheet', run_id='dataset_triggered__2022-10-12T04:34:32.203373+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'C_CSC_Balance_Sheet', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_express_merge', task_id='C_CSC_Profit_Express', run_id='dataset_triggered__2022-10-12T04:34:32.226063+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'C_CSC_Profit_Express', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,722[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_profit_notice_merge', task_id='C_CSC_Profit_Notice', run_id='dataset_triggered__2022-10-12T04:34:32.349896+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,722[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'C_CSC_Profit_Notice', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,723[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_dividend_merge', task_id='C_CSC_Dividend', run_id='dataset_triggered__2022-10-12T04:34:32.449117+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,723[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'C_CSC_Dividend', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,723[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_prices_merge', task_id='C_CSC_Prices', run_id='dataset_triggered__2022-10-12T04:34:32.485080+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:37,723[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_prices_merge', 'C_CSC_Prices', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'M_CSC_Derivative', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_income_merge', 'C_CSC_Income', 'dataset_triggered__2022-10-12T04:34:32.116082+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_cashflow_merge', 'C_CSC_CashFlow', 'dataset_triggered__2022-10-12T04:34:32.180080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_balance_sheet_merge', 'C_CSC_Balance_Sheet', 'dataset_triggered__2022-10-12T04:34:32.203373+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_express_merge', 'C_CSC_Profit_Express', 'dataset_triggered__2022-10-12T04:34:32.226063+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_profit_notice_merge', 'C_CSC_Profit_Notice', 'dataset_triggered__2022-10-12T04:34:32.349896+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_dividend_merge', 'C_CSC_Dividend', 'dataset_triggered__2022-10-12T04:34:32.449117+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,726[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_prices_merge', 'C_CSC_Prices', 'dataset_triggered__2022-10-12T04:34:32.485080+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:37,727[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_dividend_merge.M_CSC_Dividend run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,727[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_derivative_merge.T_ASHAREEODDERIVATIVEINDICATOR run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,727[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_notice_merge.M_CSC_Profit_Notice run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,727[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_express_merge.M_CSC_Profit_Express run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,727[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_balance_sheet_merge.M_CSC_Balance_Sheet run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,728[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_income_merge.M_CSC_Income run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,728[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_cashflow_merge.M_CSC_CashFlow run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,733[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_balance_sheet_merge, task_id=M_CSC_Balance_Sheet, run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.789281+00:00, run_end_date=2022-10-12 04:34:37.548051+00:00, run_duration=0.75877, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23469, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587423[0m
[[34m2022-10-12 12:34:37,733[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_cashflow_merge, task_id=M_CSC_CashFlow, run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.794549+00:00, run_end_date=2022-10-12 04:34:37.582120+00:00, run_duration=0.787571, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23471, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587424[0m
[[34m2022-10-12 12:34:37,734[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_derivative_merge, task_id=T_ASHAREEODDERIVATIVEINDICATOR, run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.828976+00:00, run_end_date=2022-10-12 04:34:37.496056+00:00, run_duration=0.66708, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23475, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587431[0m
[[34m2022-10-12 12:34:37,734[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_dividend_merge, task_id=M_CSC_Dividend, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.776715+00:00, run_end_date=2022-10-12 04:34:37.305346+00:00, run_duration=0.528631, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23468, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587421[0m
[[34m2022-10-12 12:34:37,734[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_income_merge, task_id=M_CSC_Income, run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.808768+00:00, run_end_date=2022-10-12 04:34:37.544752+00:00, run_duration=0.735984, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23473, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587425[0m
[[34m2022-10-12 12:34:37,734[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_express_merge, task_id=M_CSC_Profit_Express, run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.783725+00:00, run_end_date=2022-10-12 04:34:37.540451+00:00, run_duration=0.756726, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23470, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587422[0m
[[34m2022-10-12 12:34:37,734[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_notice_merge, task_id=M_CSC_Profit_Notice, run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.818134+00:00, run_end_date=2022-10-12 04:34:37.549984+00:00, run_duration=0.73185, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23474, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587429[0m
[[34m2022-10-12 12:34:37,815[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_prices_merge.M_CSC_Prices run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:37,818[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_prices_merge, task_id=M_CSC_Prices, run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:36.810180+00:00, run_end_date=2022-10-12 04:34:37.650447+00:00, run_duration=0.840267, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23472, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:36.547725+00:00, queued_by_job_id=23403, pid=1587426[0m
[[34m2022-10-12 12:34:37,884[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_balance_sheet_merge.C_CSC_Balance_Sheet dataset_triggered__2022-10-12T04:34:32.203373+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,898[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_prices_merge.C_CSC_Prices dataset_triggered__2022-10-12T04:34:32.485080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,909[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_income_merge.C_CSC_Income dataset_triggered__2022-10-12T04:34:32.116082+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,917[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_dividend_merge.C_CSC_Dividend dataset_triggered__2022-10-12T04:34:32.449117+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,920[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_cashflow_merge.C_CSC_CashFlow dataset_triggered__2022-10-12T04:34:32.180080+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,924[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_derivative_merge.M_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,924[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_express_merge.C_CSC_Profit_Express dataset_triggered__2022-10-12T04:34:32.226063+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:37,930[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_profit_notice_merge.C_CSC_Profit_Notice dataset_triggered__2022-10-12T04:34:32.349896+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:38,850[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_cashflow_merge @ 2022-10-12 04:34:32.180080+00:00: dataset_triggered__2022-10-12T04:34:32.180080+00:00, state:running, queued_at: 2022-10-12 04:34:32.868257+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,850[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_cashflow_merge, execution_date=2022-10-12 04:34:32.180080+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00, run_start_date=2022-10-12 04:34:33.017096+00:00, run_end_date=2022-10-12 04:34:38.850713+00:00, run_duration=5.833617, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=343f395cbf11e0f7d063d27babd72864[0m
[[34m2022-10-12 12:34:38,853[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_cashflow_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,857[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_income_merge @ 2022-10-12 04:34:32.116082+00:00: dataset_triggered__2022-10-12T04:34:32.116082+00:00, state:running, queued_at: 2022-10-12 04:34:32.899393+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,857[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_income_merge, execution_date=2022-10-12 04:34:32.116082+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00, run_start_date=2022-10-12 04:34:33.017056+00:00, run_end_date=2022-10-12 04:34:38.857673+00:00, run_duration=5.840617, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=8f3af0043363bf83125c8d3de6e2caf6[0m
[[34m2022-10-12 12:34:38,860[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_income_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,863[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_balance_sheet_merge @ 2022-10-12 04:34:32.203373+00:00: dataset_triggered__2022-10-12T04:34:32.203373+00:00, state:running, queued_at: 2022-10-12 04:34:32.890772+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,863[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_balance_sheet_merge, execution_date=2022-10-12 04:34:32.203373+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00, run_start_date=2022-10-12 04:34:33.017126+00:00, run_end_date=2022-10-12 04:34:38.863926+00:00, run_duration=5.8468, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=deea48b94bcb4f88245dc6403ab3cc74[0m
[[34m2022-10-12 12:34:38,865[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_balance_sheet_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,869[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_profit_express_merge @ 2022-10-12 04:34:32.226063+00:00: dataset_triggered__2022-10-12T04:34:32.226063+00:00, state:running, queued_at: 2022-10-12 04:34:32.847275+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,869[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_profit_express_merge, execution_date=2022-10-12 04:34:32.226063+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00, run_start_date=2022-10-12 04:34:33.017152+00:00, run_end_date=2022-10-12 04:34:38.869577+00:00, run_duration=5.852425, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=8d30e5bc685796980b268600b04b3025[0m
[[34m2022-10-12 12:34:38,871[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_profit_express_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,874[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_profit_notice_merge @ 2022-10-12 04:34:32.349896+00:00: dataset_triggered__2022-10-12T04:34:32.349896+00:00, state:running, queued_at: 2022-10-12 04:34:32.927522+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,874[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_profit_notice_merge, execution_date=2022-10-12 04:34:32.349896+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00, run_start_date=2022-10-12 04:34:33.017180+00:00, run_end_date=2022-10-12 04:34:38.874784+00:00, run_duration=5.857604, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=997acb7fafc5420c3f0b5b6dd0326cf9[0m
[[34m2022-10-12 12:34:38,876[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_profit_notice_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,879[0m] {[34mdagrun.py:[0m578} ERROR[0m - Marking run <DagRun csc_dividend_merge @ 2022-10-12 04:34:32.449117+00:00: dataset_triggered__2022-10-12T04:34:32.449117+00:00, state:running, queued_at: 2022-10-12 04:34:32.908838+00:00. externally triggered: False> failed[0m
[[34m2022-10-12 12:34:38,880[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_dividend_merge, execution_date=2022-10-12 04:34:32.449117+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, run_start_date=2022-10-12 04:34:33.017205+00:00, run_end_date=2022-10-12 04:34:38.880072+00:00, run_duration=5.862867, state=failed, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=be1875b9bc9c5a7f27e6586962c0f480[0m
[[34m2022-10-12 12:34:38,881[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_dividend_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,884[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_prices_merge @ 2022-10-12 04:34:32.485080+00:00: dataset_triggered__2022-10-12T04:34:32.485080+00:00, state:running, queued_at: 2022-10-12 04:34:32.880824+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:38,884[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_prices_merge, execution_date=2022-10-12 04:34:32.485080+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00, run_start_date=2022-10-12 04:34:33.017231+00:00, run_end_date=2022-10-12 04:34:38.884972+00:00, run_duration=5.867741, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=50727aca72d049dd0cbd199f19e57e57[0m
[[34m2022-10-12 12:34:38,886[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_prices_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:38,897[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 1 tasks up for execution:
	<TaskInstance: csc_derivative_merge.C_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:38,897[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_derivative_merge has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:34:38,898[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_derivative_merge.C_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [scheduled]>[0m
[[34m2022-10-12 12:34:38,899[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_derivative_merge', task_id='C_CSC_Derivative', run_id='dataset_triggered__2022-10-12T04:34:32.721706+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:34:38,899[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'C_CSC_Derivative', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:38,902[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_derivative_merge', 'C_CSC_Derivative', 'dataset_triggered__2022-10-12T04:34:32.721706+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_merge.py'][0m
[[34m2022-10-12 12:34:38,902[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_dividend_merge.C_CSC_Dividend run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_prices_merge.C_CSC_Prices run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_income_merge.C_CSC_Income run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_cashflow_merge.C_CSC_CashFlow run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_balance_sheet_merge.C_CSC_Balance_Sheet run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_notice_merge.C_CSC_Profit_Notice run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,903[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_profit_express_merge.C_CSC_Profit_Express run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,906[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_balance_sheet_merge, task_id=C_CSC_Balance_Sheet, run_id=dataset_triggered__2022-10-12T04:34:32.203373+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.947460+00:00, run_end_date=2022-10-12 04:34:38.677810+00:00, run_duration=0.73035, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23476, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587523[0m
[[34m2022-10-12 12:34:38,906[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_cashflow_merge, task_id=C_CSC_CashFlow, run_id=dataset_triggered__2022-10-12T04:34:32.180080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.984779+00:00, run_end_date=2022-10-12 04:34:38.658778+00:00, run_duration=0.673999, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23480, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587528[0m
[[34m2022-10-12 12:34:38,906[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_dividend_merge, task_id=C_CSC_Dividend, run_id=dataset_triggered__2022-10-12T04:34:32.449117+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.982865+00:00, run_end_date=2022-10-12 04:34:38.569786+00:00, run_duration=0.586921, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=23479, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587530[0m
[[34m2022-10-12 12:34:38,907[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_income_merge, task_id=C_CSC_Income, run_id=dataset_triggered__2022-10-12T04:34:32.116082+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.974934+00:00, run_end_date=2022-10-12 04:34:38.620220+00:00, run_duration=0.645286, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23478, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587526[0m
[[34m2022-10-12 12:34:38,907[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_prices_merge, task_id=C_CSC_Prices, run_id=dataset_triggered__2022-10-12T04:34:32.485080+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.958286+00:00, run_end_date=2022-10-12 04:34:38.581655+00:00, run_duration=0.623369, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23477, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587524[0m
[[34m2022-10-12 12:34:38,907[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_express_merge, task_id=C_CSC_Profit_Express, run_id=dataset_triggered__2022-10-12T04:34:32.226063+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.982303+00:00, run_end_date=2022-10-12 04:34:38.688939+00:00, run_duration=0.706636, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23482, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587527[0m
[[34m2022-10-12 12:34:38,907[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_profit_notice_merge, task_id=C_CSC_Profit_Notice, run_id=dataset_triggered__2022-10-12T04:34:32.349896+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.996755+00:00, run_end_date=2022-10-12 04:34:38.675075+00:00, run_duration=0.67832, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23483, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587531[0m
[[34m2022-10-12 12:34:38,943[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_derivative_merge.M_CSC_Derivative run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:38,945[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_derivative_merge, task_id=M_CSC_Derivative, run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00, map_index=-1, run_start_date=2022-10-12 04:34:37.984225+00:00, run_end_date=2022-10-12 04:34:38.833037+00:00, run_duration=0.848812, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23481, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:37.720179+00:00, queued_by_job_id=23403, pid=1587529[0m
[[34m2022-10-12 12:34:39,010[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_derivative_merge.C_CSC_Derivative dataset_triggered__2022-10-12T04:34:32.721706+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:34:39,970[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_derivative_merge @ 2022-10-12 04:34:32.721706+00:00: dataset_triggered__2022-10-12T04:34:32.721706+00:00, state:running, queued_at: 2022-10-12 04:34:32.918123+00:00. externally triggered: False> successful[0m
[[34m2022-10-12 12:34:39,970[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_derivative_merge, execution_date=2022-10-12 04:34:32.721706+00:00, run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00, run_start_date=2022-10-12 04:34:33.017256+00:00, run_end_date=2022-10-12 04:34:39.970505+00:00, run_duration=6.953249, state=success, external_trigger=False, run_type=dataset_triggered, data_interval_start=2022-10-10 17:00:00+00:00, data_interval_end=2022-10-11 17:00:00+00:00, dag_hash=9bc836663ed89f5e192bc8ad2edc0c52[0m
[[34m2022-10-12 12:34:39,971[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_derivative_merge to None, run_after=None[0m
[[34m2022-10-12 12:34:39,978[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_derivative_merge.C_CSC_Derivative run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:34:39,980[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_derivative_merge, task_id=C_CSC_Derivative, run_id=dataset_triggered__2022-10-12T04:34:32.721706+00:00, map_index=-1, run_start_date=2022-10-12 04:34:39.049699+00:00, run_end_date=2022-10-12 04:34:39.436629+00:00, run_duration=0.38693, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23484, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:34:38.898255+00:00, queued_by_job_id=23403, pid=1587635[0m
Process ForkProcess-60:
Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 555, in cmd_query
    self._cmysql.query(
_mysql_connector.MySQLInterfaceError: Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
mysql.connector.errors.OperationalError: 2013 (HY000): Lost connection to MySQL server during query

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 257, in _run_processor_manager
    processor_manager.start()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 603, in _run_parsing_loop
    DagWarning.purge_inactive_dag_warnings()
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/airflow/models/dagwarning.py", line 82, in purge_inactive_dag_warnings
    query.delete(synchronize_session=False)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/query.py", line 3213, in delete
    result = self.session.execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/orm/session.py", line 1712, in execute
    result = conn._execute_20(statement, params or {}, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1705, in _execute_20
    return meth(self, args_10style, kwargs_10style, execution_options)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/sql/elements.py", line 333, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1572, in _execute_clauseelement
    ret = self._execute_context(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1943, in _execute_context
    self._handle_dbapi_exception(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 2124, in _handle_dbapi_exception
    util.raise_(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/util/compat.py", line 208, in raise_
    raise exception
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/base.py", line 1900, in _execute_context
    self.dialect.do_execute(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/sqlalchemy/engine/default.py", line 736, in do_execute
    cursor.execute(statement, parameters)
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/cursor_cext.py", line 276, in execute
    result = self._cnx.cmd_query(
  File "/home/lianghua/anaconda3/envs/zirui_env/lib/python3.9/site-packages/mysql/connector/connection_cext.py", line 563, in cmd_query
    raise get_mysql_exception(
sqlalchemy.exc.OperationalError: (mysql.connector.errors.OperationalError) 2013 (HY000): Lost connection to MySQL server during query
[SQL: DELETE FROM dag_warning USING dag_warning, dag WHERE dag_warning.dag_id = dag.dag_id AND dag.is_active = false]
(Background on this error at: https://sqlalche.me/e/14/e3q8)
[[34m2022-10-12 12:36:20,947[0m] {[34mmanager.py:[0m288} WARNING[0m - DagFileProcessorManager (PID=1187449) exited with exit code 1 - re-launching[0m
[[34m2022-10-12 12:36:20,949[0m] {[34mmanager.py:[0m163} INFO[0m - Launched DagFileProcessorManager with pid: 1601619[0m
[[34m2022-10-12 12:36:20,953[0m] {[34msettings.py:[0m58} INFO[0m - Configured default timezone Timezone('UTC')[0m
[[34m2022-10-12 12:36:29,098[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:41:29,126[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:46:29,160[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:51:29,196[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:56:29,230[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2022-10-12 12:56:30,161[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 2 tasks up for execution:
	<TaskInstance: csc_ops_update.get_data_list manual__2022-10-12T04:56:29.486716+00:00 [scheduled]>
	<TaskInstance: csc_ops_update.get_table_list manual__2022-10-12T04:56:29.486716+00:00 [scheduled]>[0m
[[34m2022-10-12 12:56:30,161[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:56:30,161[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:56:30,161[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_update.get_data_list manual__2022-10-12T04:56:29.486716+00:00 [scheduled]>
	<TaskInstance: csc_ops_update.get_table_list manual__2022-10-12T04:56:29.486716+00:00 [scheduled]>[0m
[[34m2022-10-12 12:56:30,162[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='get_data_list', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:56:30,163[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'get_data_list', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py'][0m
[[34m2022-10-12 12:56:30,163[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='get_table_list', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2022-10-12 12:56:30,163[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'get_table_list', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py'][0m
[[34m2022-10-12 12:56:30,166[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'get_data_list', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py'][0m
[[34m2022-10-12 12:56:30,166[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'get_table_list', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py'][0m
[[34m2022-10-12 12:56:30,272[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.get_data_list manual__2022-10-12T04:56:29.486716+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:30,272[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.get_table_list manual__2022-10-12T04:56:29.486716+00:00 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:31,235[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 4 tasks up for execution:
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [scheduled]>[0m
[[34m2022-10-12 12:56:31,236[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:56:31,236[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:56:31,236[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:56:31,236[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 3/16 running and queued tasks[0m
[[34m2022-10-12 12:56:31,236[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [scheduled]>
	<TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [scheduled]>[0m
[[34m2022-10-12 12:56:31,237[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='extract_sql_by_table', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=0) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:56:31,237[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '0'][0m
[[34m2022-10-12 12:56:31,238[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='extract_sql_by_table', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=1) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:56:31,238[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '1'][0m
[[34m2022-10-12 12:56:31,238[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='extract_sql_by_table', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=2) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:56:31,238[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '2'][0m
[[34m2022-10-12 12:56:31,238[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='extract_sql_by_table', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=3) to executor with priority 2 and queue default[0m
[[34m2022-10-12 12:56:31,238[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '3'][0m
[[34m2022-10-12 12:56:31,241[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '0'][0m
[[34m2022-10-12 12:56:31,241[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '1'][0m
[[34m2022-10-12 12:56:31,241[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '2'][0m
[[34m2022-10-12 12:56:31,241[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'extract_sql_by_table', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '3'][0m
[[34m2022-10-12 12:56:31,242[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.get_data_list run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:31,242[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.get_table_list run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:31,245[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=get_data_list, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=-1, run_start_date=2022-10-12 04:56:30.313070+00:00, run_end_date=2022-10-12 04:56:30.582588+00:00, run_duration=0.269518, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23485, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:30.161687+00:00, queued_by_job_id=23403, pid=1780580[0m
[[34m2022-10-12 12:56:31,245[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=get_table_list, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=-1, run_start_date=2022-10-12 04:56:30.313066+00:00, run_end_date=2022-10-12 04:56:30.582135+00:00, run_duration=0.269069, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23486, pool=default_pool, queue=default, priority_weight=3, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:30.161687+00:00, queued_by_job_id=23403, pid=1780581[0m
[[34m2022-10-12 12:56:31,353[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:31,354[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:31,365[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:31,378[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.extract_sql_by_table manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m347} INFO[0m - 4 tasks up for execution:
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [scheduled]>[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 0/16 running and queued tasks[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 1/16 running and queued tasks[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 2/16 running and queued tasks[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m412} INFO[0m - DAG csc_ops_update has 3/16 running and queued tasks[0m
[[34m2022-10-12 12:56:32,323[0m] {[34mscheduler_job.py:[0m498} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [scheduled]>
	<TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [scheduled]>[0m
[[34m2022-10-12 12:56:32,324[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='load_sql_query', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=0) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:56:32,324[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '0'][0m
[[34m2022-10-12 12:56:32,324[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='load_sql_query', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=1) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:56:32,324[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '1'][0m
[[34m2022-10-12 12:56:32,324[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='load_sql_query', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=2) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:56:32,325[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '2'][0m
[[34m2022-10-12 12:56:32,325[0m] {[34mscheduler_job.py:[0m537} INFO[0m - Sending TaskInstanceKey(dag_id='csc_ops_update', task_id='load_sql_query', run_id='manual__2022-10-12T04:56:29.486716+00:00', try_number=1, map_index=3) to executor with priority 1 and queue default[0m
[[34m2022-10-12 12:56:32,325[0m] {[34mbase_executor.py:[0m95} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '3'][0m
[[34m2022-10-12 12:56:32,327[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '0'][0m
[[34m2022-10-12 12:56:32,327[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '1'][0m
[[34m2022-10-12 12:56:32,327[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '2'][0m
[[34m2022-10-12 12:56:32,327[0m] {[34mlocal_executor.py:[0m81} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'csc_ops_update', 'load_sql_query', 'manual__2022-10-12T04:56:29.486716+00:00', '--local', '--subdir', 'DAGS_FOLDER/zirui_dag/csc_ops_update.py', '--map-index', '3'][0m
[[34m2022-10-12 12:56:32,327[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.extract_sql_by_table run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:32,328[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.extract_sql_by_table run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:32,328[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.extract_sql_by_table run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:32,328[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.extract_sql_by_table run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:32,333[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=extract_sql_by_table, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=0, run_start_date=2022-10-12 04:56:31.409183+00:00, run_end_date=2022-10-12 04:56:31.819042+00:00, run_duration=0.409859, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23489, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:31.236520+00:00, queued_by_job_id=23403, pid=1780740[0m
[[34m2022-10-12 12:56:32,333[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=extract_sql_by_table, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=1, run_start_date=2022-10-12 04:56:31.400320+00:00, run_end_date=2022-10-12 04:56:31.792087+00:00, run_duration=0.391767, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23488, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:31.236520+00:00, queued_by_job_id=23403, pid=1780739[0m
[[34m2022-10-12 12:56:32,333[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=extract_sql_by_table, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=2, run_start_date=2022-10-12 04:56:31.396774+00:00, run_end_date=2022-10-12 04:56:31.798956+00:00, run_duration=0.402182, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23487, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:31.236520+00:00, queued_by_job_id=23403, pid=1780738[0m
[[34m2022-10-12 12:56:32,333[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=extract_sql_by_table, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=3, run_start_date=2022-10-12 04:56:31.426188+00:00, run_end_date=2022-10-12 04:56:31.818126+00:00, run_duration=0.391938, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23490, pool=default_pool, queue=default, priority_weight=2, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:31.236520+00:00, queued_by_job_id=23403, pid=1780744[0m
[[34m2022-10-12 12:56:32,444[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=2 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:32,444[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=0 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:32,450[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=1 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:32,471[0m] {[34mtask_command.py:[0m384} INFO[0m - Running <TaskInstance: csc_ops_update.load_sql_query manual__2022-10-12T04:56:29.486716+00:00 map_index=3 [queued]> on host administrator-OptiPlex-7090[0m
[[34m2022-10-12 12:56:33,410[0m] {[34mdagrun.py:[0m597} INFO[0m - Marking run <DagRun csc_ops_update @ 2022-10-12 04:56:29.486716+00:00: manual__2022-10-12T04:56:29.486716+00:00, state:running, queued_at: 2022-10-12 04:56:29.495304+00:00. externally triggered: True> successful[0m
[[34m2022-10-12 12:56:33,410[0m] {[34mdagrun.py:[0m644} INFO[0m - DagRun Finished: dag_id=csc_ops_update, execution_date=2022-10-12 04:56:29.486716+00:00, run_id=manual__2022-10-12T04:56:29.486716+00:00, run_start_date=2022-10-12 04:56:30.138886+00:00, run_end_date=2022-10-12 04:56:33.410338+00:00, run_duration=3.271452, state=success, external_trigger=True, run_type=manual, data_interval_start=2022-10-12 04:56:29.486716+00:00, data_interval_end=2022-10-12 04:56:29.486716+00:00, dag_hash=b42781936c4c4ec2206441d50f863caf[0m
[[34m2022-10-12 12:56:33,412[0m] {[34mdag.py:[0m3324} INFO[0m - Setting next_dagrun for csc_ops_update to None, run_after=None[0m
[[34m2022-10-12 12:56:33,418[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.load_sql_query run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:33,418[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.load_sql_query run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:33,418[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.load_sql_query run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:33,418[0m] {[34mscheduler_job.py:[0m589} INFO[0m - Executor reports execution of csc_ops_update.load_sql_query run_id=manual__2022-10-12T04:56:29.486716+00:00 exited with status success for try_number 1[0m
[[34m2022-10-12 12:56:33,420[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=load_sql_query, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=0, run_start_date=2022-10-12 04:56:32.493163+00:00, run_end_date=2022-10-12 04:56:33.158744+00:00, run_duration=0.665581, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23492, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:32.323702+00:00, queued_by_job_id=23403, pid=1780920[0m
[[34m2022-10-12 12:56:33,420[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=load_sql_query, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=1, run_start_date=2022-10-12 04:56:32.495217+00:00, run_end_date=2022-10-12 04:56:33.148027+00:00, run_duration=0.65281, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23493, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:32.323702+00:00, queued_by_job_id=23403, pid=1780921[0m
[[34m2022-10-12 12:56:33,420[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=load_sql_query, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=2, run_start_date=2022-10-12 04:56:32.490241+00:00, run_end_date=2022-10-12 04:56:33.165703+00:00, run_duration=0.675462, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23491, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:32.323702+00:00, queued_by_job_id=23403, pid=1780915[0m
[[34m2022-10-12 12:56:33,420[0m] {[34mscheduler_job.py:[0m632} INFO[0m - TaskInstance Finished: dag_id=csc_ops_update, task_id=load_sql_query, run_id=manual__2022-10-12T04:56:29.486716+00:00, map_index=3, run_start_date=2022-10-12 04:56:32.521129+00:00, run_end_date=2022-10-12 04:56:33.184756+00:00, run_duration=0.663627, state=success, executor_state=success, try_number=1, max_tries=0, job_id=23494, pool=default_pool, queue=default, priority_weight=1, operator=_PythonDecoratedOperator, queued_dttm=2022-10-12 04:56:32.323702+00:00, queued_by_job_id=23403, pid=1780928[0m
[[34m2022-10-12 13:01:29,258[0m] {[34mscheduler_job.py:[0m1369} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2022-10-12 13:02:13 +0800] [707815] [INFO] Handling signal: hup
[2022-10-12 13:02:13 +0800] [707815] [INFO] Hang up: Master
[2022-10-12 13:02:13 +0800] [707815] [WARNING] Worker with pid 707816 was terminated due to signal 1
[2022-10-12 13:02:13 +0800] [707815] [WARNING] Worker with pid 707877 was terminated due to signal 1
[2022-10-12 13:02:13 +0800] [1843595] [INFO] Booting worker with pid: 1843595
[2022-10-12 13:02:13 +0800] [1843601] [INFO] Booting worker with pid: 1843601
